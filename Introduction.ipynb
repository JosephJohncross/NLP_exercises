{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, their Labels, and their Positions\n",
      "T1 (CARDINAL)\n",
      "China (GPE)\n",
      "T1 (ORG)\n",
      "AI (ORG)\n",
      "DeepSeek (PRODUCT)\n",
      "Western (NORP)\n",
      "T1 (ORG)\n",
      "Tencent (ORG)\n",
      "Turbo S (ORG)\n",
      "last month (DATE)\n",
      "DeepSeek (PRODUCT)\n",
      "AI (ORG)\n",
      "2025 (DATE)\n",
      "2024 (DATE)\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition using spacy322323\n",
    "\\\n",
    "\\\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"Tencent has launched the official version of its T1 reasoning model, enhancing its position in China's competitive artificial intelligence sector. \n",
    "The T1 model boasts improved response times and extended text processing capabilities, maintaining clear content logic and a low hallucination rate. \n",
    "This move follows the introduction of competitive AI models by DeepSeek, which offer comparable performance to Western systems at lower costs. \n",
    "The T1 model benefits from Tencent's Turbo S foundational language model, introduced last month, which the company claims is faster than DeepSeek's R1. \n",
    "Tencent has increased its AI investments and plans further capital expenditure in 2025 following significant spending in 2024.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Named Entities, their Labels, and their Positions\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: The battery life of this phone is amazing! It lasts two days easily.\n",
      "Sentiment: Positive (Score 0.59)\n",
      "\n",
      "Review: The camera quality is terrible. The pictures are always blurry.\n",
      "Sentiment: Negative (Score -1.00)\n",
      "\n",
      "Review: Fast performance and a great display, but the speaker quality is below average.\n",
      "Sentiment: Positive (Score 0.28)\n",
      "\n",
      "Review: The laptop heats up too quickly, making it hard to use for long hours.\n",
      "Sentiment: Negative (Score -0.00)\n",
      "\n",
      "Review: Absolutely love this smartwatch! The features are just what I needed.\n",
      "Sentiment: Positive (Score 0.62)\n",
      "\n",
      "Review: The product is below average and fairly matches the picture\n",
      "Sentiment: Positive (Score 0.27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "reviews = [\n",
    "    \"The battery life of this phone is amazing! It lasts two days easily.\",\n",
    "    \"The camera quality is terrible. The pictures are always blurry.\",\n",
    "    \"Fast performance and a great display, but the speaker quality is below average.\",\n",
    "    \"The laptop heats up too quickly, making it hard to use for long hours.\",\n",
    "    \"Absolutely love this smartwatch! The features are just what I needed.\",\n",
    "    \"The product is below average and fairly matches the picture\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    sentiment_score = TextBlob(review).sentiment.polarity\n",
    "    sentiment = \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n",
    "\n",
    "    print(f\"Review: {review}\\nSentiment: {sentiment} (Score {sentiment_score:.2f})\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'big', 'brown', 'fox', 'jumped', 'over', 'the', 'window', '!', '.', 'But', 'notice', 'that', 'the', 'cow', 'is', 'running', 'towards', 'an', 'ajar', 'door', 'that', 'was', 'made', 'of', 'a', 'rich', 'blend', 'of', 'cinammon', 'and', 'oak']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using NLTK\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# txt\n",
    "text = \"The big brown fox jumped over the window!. But  notice that the cow is running towards an ajar door that was made of a rich blend of cinammon and oak\"\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['big', 'brown', 'fox', 'jumped', 'window', '!', '.', 'notice', 'cow', 'running', 'towards', 'ajar', 'door', 'made', 'rich', 'blend', 'cinammon', 'oak']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print('Filtered Tokens:', filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed tokens ['big', 'brown', 'fox', 'jump', 'window', '!', '.', 'notic', 'cow', 'run', 'toward', 'ajar', 'door', 'made', 'rich', 'blend', 'cinammon', 'oak']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"stemmed tokens\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Token: ['big', 'brown', 'fox', 'jumped', 'window', '!', '.', 'notice', 'cow', 'running', 'towards', 'ajar', 'door', 'made', 'rich', 'blend', 'cinammon', 'oak']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/joe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lematization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatization_token = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print('Lemmatization Token:', lemmatization_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Tokens: ['big', 'brown', 'fox', 'jumped', 'window', '!', '.', 'notice', 'cow', 'running', 'towards', 'ajar', 'door', 'made', 'rich', 'blend', 'cinammon', 'oak']\n"
     ]
    }
   ],
   "source": [
    "# Case Conversion\n",
    "\n",
    "lowercase_tokens = [token.lower() for token in lemmatization_token]\n",
    "print('Lowercase Tokens:', lowercase_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
