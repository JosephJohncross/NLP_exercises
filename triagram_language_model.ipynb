{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An N-gram model uses the frequency of N-grams in a text to calculate the probabiliy of he next word in a sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a triagram based language model \n",
    "Why?\n",
    "\n",
    "Cause Trigram provides a balance betweeen simplicity and contextual accuracy by considering two precding words.\n",
    "\n",
    "## Objective\n",
    "1. Create a trigram lang model from  a given corpus\n",
    "2. Use the model to predict the nxt word in a given sequence\n",
    "3. Evaluate the model performance using perplexity as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict, Counter\n",
    "import math \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/joe/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /home/joe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"gutenberg\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a sample text\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "text = gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1629\n",
      "Sample sentences: ['[', 'alice', \"'\", 's', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', '1865', ']', 'chapter', 'i', '.']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the text: Lowrrcase and tokenization\n",
    "\n",
    "sentences = nltk.sent_tokenize(' '.join(text))\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "print(f\"Number of sentences: {len(tokenized_sentences)}\")\n",
    "print(f\"Sample sentences: {tokenized_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the trigram model\n",
    "### Steps\n",
    "1. Generate the trigrams from the text\n",
    "2. Count the occurence of each trigram and bigram\n",
    "3. Calculate the conditional probability of each trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams: 14163\n",
      "Number of unique trigrams: 13932\n"
     ]
    }
   ],
   "source": [
    "# Generate trigram and count their frequncies\n",
    "\n",
    "trigram_count = defaultdict(Counter)\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    trigrams = list(ngrams(sentence, 3))\n",
    "    bigrams = list(ngrams(sentence, 2))\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        trigram_count[(trigram[0], trigram[1])][trigram[2]] += 1\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts[bigram] += 1\n",
    "\n",
    "print(f\"Number of unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"Number of unique trigrams: {len(trigram_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given words: 'alice was', Predicted next word: 'not'\n"
     ]
    }
   ],
   "source": [
    "# Predicting the next word\n",
    "\n",
    "def predict_next_word(w1, w2, trigram_counts):\n",
    "    \"\"\"Predict the next word, based on tigram probabilities\"\"\"\n",
    "\n",
    "    next_words_probs = trigram_counts.get((w1, w2), {})\n",
    "    if not next_words_probs:\n",
    "        return  \"Unknown\"\n",
    "    return max(next_words_probs, key=next_words_probs.get)\n",
    "\n",
    "\n",
    "w1, w2 = \"alice\", \"was\"\n",
    "precidted_word = predict_next_word(w1, w2, trigram_count)\n",
    "\n",
    "print(f\"Given words: '{w1} {w2}', Predicted next word: '{precidted_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the trigram model:  2.923313470850842\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model with perplexity\n",
    "\n",
    "def calculate_perpleity(test_sentences, trigram_counts, bigram_counts):\n",
    "    \"\"\"Calculate the perplexity of the trigram model on test data.\"\"\"\n",
    "\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        trigrams = list(ngrams(sentence, 3))\n",
    "        for trigram in trigrams:\n",
    "            w1, w2, w3 = trigram\n",
    "            trigram_prob = (trigram_counts[(w1, w2)][w3] / bigram_counts[(w1, w2)]\n",
    "                            if (w1, w2) in bigram_counts else 1e-6)\n",
    "            total_log_prob += math.log2(trigram_prob)\n",
    "            total_words += 1\n",
    "\n",
    "    perplexity = 2 ** (-total_log_prob / total_words)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Evaluate the model on a subset of the data\n",
    "test_sentences = tokenized_sentences[:100]\n",
    "perplexity = calculate_perpleity(test_sentences, trigram_count, bigram_counts)\n",
    "print(f\"Perplexity of the trigram model:  {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impovement of the above code\n",
    "\n",
    "\n",
    "# def calculate_perplexity(test_sentences, trigram_counts, bigram_counts, vocab_size, alpha=1.0):\n",
    "#     \"\"\"\n",
    "#     Calculate the perplexity of a trigram language model using Laplace smoothing.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - test_sentences: List of tokenized sentences\n",
    "#     - trigram_counts: Dictionary of trigram frequencies\n",
    "#     - bigram_counts: Dictionary of bigram frequencies\n",
    "#     - vocab_size: Total number of unique words in the dataset\n",
    "#     - alpha: Smoothing parameter (default=1 for Laplace smoothing)\n",
    "    \n",
    "#     Returns:\n",
    "#     - perplexity: Measure of how well the model predicts unseen data\n",
    "#     \"\"\"\n",
    "\n",
    "#     total_log_prob = 0\n",
    "#     total_words = 0\n",
    "\n",
    "#     for sentence in test_sentences:\n",
    "#         trigrams = list(ngrams(sentence, 3))\n",
    "\n",
    "#         for trigram in trigrams:\n",
    "#             w1, w2, w3 = trigram\n",
    "            \n",
    "#             # Apply Laplace Smoothing to avoid zero probabilities\n",
    "#             trigram_freq = trigram_counts[(w1, w2)][w3] + alpha\n",
    "#             bigram_freq = bigram_counts[(w1, w2)] + (alpha * vocab_size)\n",
    "\n",
    "#             trigram_prob = trigram_freq / bigram_freq  # Smoothed probability\n",
    "\n",
    "#             total_log_prob += math.log2(trigram_prob)\n",
    "#             total_words += 1\n",
    "\n",
    "#     # Prevent division by zero\n",
    "#     if total_words == 0:\n",
    "#         return float('inf')\n",
    "\n",
    "#     perplexity = 2 ** (-total_log_prob / total_words)\n",
    "#     return perplexity\n",
    "\n",
    "# # Example Usage:\n",
    "# test_sentences = tokenized_sentences[:100]\n",
    "# vocab_size = len(set(word for sentence in tokenized_sentences for word in sentence))  # Unique word count\n",
    "# perplexity = calculate_perplexity(test_sentences, trigram_count, bigram_counts, vocab_size)\n",
    "\n",
    "# print(f\"Perplexity of the improved trigram model: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
